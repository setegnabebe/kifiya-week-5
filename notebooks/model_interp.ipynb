{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import shap\n",
    "import lime\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "from transformers import (pipeline, AutoTokenizer, AutoModelForTokenClassification)\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sa/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the model and tokenizer (for interpretability, choose one model, e.g., XLM-Roberta)\n",
    "model_name = \"xlm-roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All NaN and empty values have been cleaned from the texts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input type: <class 'list'>, Content: ['ğŸ’¥3pcs silicon brush spatulas\\n\\nâš¡áŠ¥áˆµáŠ¨ 260Â°c áˆ™á‰€á‰µ áˆ˜á‰†á‰†áˆ á‹¨áˆšá‰½áˆ\\n\\xa0\\xa0\\xa0\\xa0\\xa0 \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 á‹‹áŒ‹-550á‰¥áˆ­âœ…\\n\\nğŸ¢ áŠ á‹µáˆ«áˆ»\\xa0 á‰.1ğŸ‘‰ áˆµáˆª áŠ¤áˆ áˆ²á‰² áˆáˆ\\xa0 áˆáˆˆá‰°áŠ› áá‰… á‰¢áˆ® á‰. SL-05A(áŠ¨ áˆŠáá‰± áŠá‰µ áˆˆ áŠá‰µ)\\n\\nğŸ“á‰.2 ğŸ‘‰áˆˆá‰¡\\xa0 áˆ˜á‹³áˆ…áŠ’á‹“áˆˆáˆ á‰¤á‰°/áŠ­áˆ­áˆµá‰²á‹«áŠ• áŠá‰µ áˆˆáŠá‰µ\\xa0 #á‹›áˆ_áˆáˆ 2áŠ› áá‰… á‰¢áˆ® á‰áŒ¥áˆ­.214\\n\\nğŸ‘áˆˆá‰¡\\xa0á‰…áˆ­áŠ•áŒ«áğŸ“²0973611819\\n\\n\\n\\n\\xa0\\xa0\\xa0 ğŸ“² 0909522840\\n\\xa0\\xa0\\xa0 ğŸ“² 0923350054\\n\\nğŸ”–\\nğŸ’¬\\xa0 á‰ Telegram áˆˆáˆ›á‹˜á‹ â¤µï¸ á‹­áŒ á‰€áˆ™\\n@shager_onlinestore\\n\\xa0 \\náˆˆá‰°áŒ¨áˆ›áˆª áˆ›á‰¥áˆ«áˆªá‹« á‹¨á‰´áˆŒáŒáˆ«áˆ áŒˆáƒá‰½áŠ•â¤µï¸\\nhttps://t.me/Shageronlinestore', 'ğŸ’¥Mandoline Slicer\\n\\nğŸ‘‰ áŒŠá‹œ á‰†áŒ£á‰¢ áˆµáˆ‹á‹­áˆµ áˆ›á‹µáˆ¨áŒŠá‹« \\nğŸ‘‰\\xa0 áˆˆáŠ¥áŒ… áˆ´áá‰² á‰°áˆ˜áˆ«áŒ­\\nğŸ‘‰\\xa0 áˆˆá‹µáŠ•á‰½ áˆˆáŠ«áˆ®á‰µáŠ“ áˆŒáˆá‰½ áŠ á‰³áŠ­áˆá‰¶á‰½ á‰°áˆ˜áˆ«áŒ­ \\nğŸ‘‰áŒ¥áˆ«á‰µ á‹«áˆˆá‹ á‹•á‰ƒ\\n\\n\\xa0\\xa0\\xa0  á‹‹áŒ‹á¦ âœ… 1,200 á‰¥áˆ­\\n\\nğŸ¢ áŠ á‹µáˆ«áˆ»\\xa0 á‰.1ğŸ‘‰ áˆµáˆª áŠ¤áˆ áˆ²á‰² áˆáˆ\\xa0 áˆáˆˆá‰°áŠ› áá‰… á‰¢áˆ® á‰. SL-05A(áŠ¨ áˆŠáá‰± áŠá‰µ áˆˆ áŠá‰µ)\\n\\nğŸ“á‰.2 ğŸ‘‰áˆˆá‰¡\\xa0 áˆ˜á‹³áˆ…áŠ’á‹“áˆˆáˆ á‰¤á‰°/áŠ­áˆ­áˆµá‰²á‹«áŠ• áŠá‰µ áˆˆáŠá‰µ\\xa0 #á‹›áˆ_áˆáˆ 2áŠ› áá‰… á‰¢áˆ® á‰áŒ¥áˆ­.214\\n\\nğŸ‘áˆˆá‰¡\\xa0á‰…áˆ­áŠ•áŒ«áğŸ“²0973611819\\n\\n\\n\\n\\xa0\\xa0\\xa0 ğŸ“² 0909522840\\n\\xa0\\xa0\\xa0 ğŸ“² 0923350054\\n\\nğŸ”–\\nğŸ’¬\\xa0 á‰ Telegram áˆˆáˆ›á‹˜á‹ â¤µï¸ á‹­áŒ á‰€áˆ™\\n@shager_onlinestore\\n\\xa0 \\náˆˆá‰°áŒ¨áˆ›áˆª áˆ›á‰¥áˆ«áˆªá‹« á‹¨á‰´áˆŒáŒáˆ«áˆ áŒˆáƒá‰½áŠ•â¤µï¸\\nhttps://t.me/Shageronlinestore', 'ğŸ’¥Table Desk Edge Guard Strip\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 ğŸ’¯ High Quality \\n\\nğŸ‘‰Made from soft, environmentally friendly, fireproof and non-toxic material.\\n\\nğŸ‘‰Help to protect your child from the sharp edges and corners at home\\nğŸ‘‰Suitable for wood, bakelite, ceramic tile, marble, glass, metal corner, etc\\nğŸ‘‰Total length 2m, suitable for different sizes of furniture.\\n\\n#Package_Included:\\n1* Table Desk Edge Guard Strip\\n1 * Double-sided Adhesive\\n\\ná‹‹áŒ‹á¦\\xa0 ğŸ’°ğŸ·\\xa0 2meter:- 500 á‰¥áˆ­âœ…\\n\\nâ™¦ï¸á‹áˆµáŠ• ááˆ¬ áŠá‹ á‹«áˆˆá‹\\n\\nğŸ¢ áŠ á‹µáˆ«áˆ»\\xa0 á‰.1ğŸ‘‰ áˆµáˆª áŠ¤áˆ áˆ²á‰² áˆáˆ\\xa0 áˆáˆˆá‰°áŠ› áá‰… á‰¢áˆ® á‰. SL-05A(áŠ¨ áˆŠáá‰± áŠá‰µ áˆˆ áŠá‰µ)\\n\\nğŸ“á‰.2 ğŸ‘‰áˆˆá‰¡\\xa0 áˆ˜á‹³áˆ…áŠ’á‹“áˆˆáˆ á‰¤á‰°/áŠ­áˆ­áˆµá‰²á‹«áŠ• áŠá‰µ áˆˆáŠá‰µ\\xa0 #á‹›áˆ_áˆáˆ 2áŠ› áá‰… á‰¢áˆ® á‰áŒ¥áˆ­.214\\n\\nğŸ‘áˆˆá‰¡\\xa0á‰…áˆ­áŠ•áŒ«áğŸ“²0973611819\\n\\n\\n\\xa0\\xa0\\xa0\\xa0 ğŸ’§ğŸ’§ğŸ’§ğŸ’§\\n\\n\\n\\xa0\\xa0\\xa0 ğŸ“² 0909522840\\n\\xa0\\xa0\\xa0 ğŸ“² 0923350054\\n\\nğŸ”–\\nğŸ’¬\\xa0 á‰ Telegram áˆˆáˆ›á‹˜á‹ â¤µï¸ á‹­áŒ á‰€áˆ™\\n@shager_onlinestore\\n\\xa0 \\náˆˆá‰°áŒ¨áˆ›áˆª áˆ›á‰¥áˆ«áˆªá‹« á‹¨á‰´áˆŒáŒáˆ«áˆ áŒˆáƒá‰½áŠ•â¤µï¸\\nhttps://t.me/Shageronlinestore', 'ğŸ’¥Table Desk Edge Guard Strip\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 ğŸ’¯ High Quality \\n\\nğŸ‘‰Made from soft, environmentally friendly, fireproof and non-toxic material.\\n\\nğŸ‘‰Help to protect your child from the sharp edges and corners at home\\nğŸ‘‰Suitable for wood, bakelite, ceramic tile, marble, glass, metal corner, etc\\nğŸ‘‰Total length 2m, suitable for different sizes of furniture.\\n\\n#Package_Included:\\n1* Table Desk Edge Guard Strip\\n1 * Double-sided Adhesive\\n\\ná‹‹áŒ‹á¦\\xa0 ğŸ’°ğŸ·\\xa0 2meter:- 500 á‰¥áˆ­âœ…\\n\\nâ™¦ï¸á‹áˆµáŠ• ááˆ¬ áŠá‹ á‹«áˆˆá‹\\n\\nğŸ¢ áŠ á‹µáˆ«áˆ»\\xa0 á‰.1ğŸ‘‰ áˆµáˆª áŠ¤áˆ áˆ²á‰² áˆáˆ\\xa0 áˆáˆˆá‰°áŠ› áá‰… á‰¢áˆ® á‰. SL-05A(áŠ¨ áˆŠáá‰± áŠá‰µ áˆˆ áŠá‰µ)\\n\\nğŸ“á‰.2 ğŸ‘‰áˆˆá‰¡\\xa0 áˆ˜á‹³áˆ…áŠ’á‹“áˆˆáˆ á‰¤á‰°/áŠ­áˆ­áˆµá‰²á‹«áŠ• áŠá‰µ áˆˆáŠá‰µ\\xa0 #á‹›áˆ_áˆáˆ 2áŠ› áá‰… á‰¢áˆ® á‰áŒ¥áˆ­.214\\n\\nğŸ‘áˆˆá‰¡\\xa0á‰…áˆ­áŠ•áŒ«áğŸ“²0973611819\\n\\n\\n\\xa0\\xa0\\xa0\\xa0 ğŸ’§ğŸ’§ğŸ’§ğŸ’§\\n\\n\\n\\xa0\\xa0\\xa0 ğŸ“² 0909522840\\n\\xa0\\xa0\\xa0 ğŸ“² 0923350054\\n\\nğŸ”–\\nğŸ’¬\\xa0 á‰ Telegram áˆˆáˆ›á‹˜á‹ â¤µï¸ á‹­áŒ á‰€áˆ™\\n@shager_onlinestore\\n\\xa0 \\náˆˆá‰°áŒ¨áˆ›áˆª áˆ›á‰¥áˆ«áˆªá‹« á‹¨á‰´áˆŒáŒáˆ«áˆ áŒˆáƒá‰½áŠ•â¤µï¸\\nhttps://t.me/Shageronlinestore', 'ğŸ’¥Only baby 3in1 double bottle milk warmer,sterilizer,food steamer\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 á‹‹áŒ‹:-3000á‰¥áˆ­âœ…\\n\\nâŒá‹áˆµáŠ• ááˆ¬ áŠá‹ á‹«áˆˆá‹\\n\\nğŸ¢ áŠ á‹µáˆ«áˆ»\\xa0 á‰.1ğŸ‘‰ áˆµáˆª áŠ¤áˆ áˆ²á‰² áˆáˆ\\xa0 áˆáˆˆá‰°áŠ› áá‰… á‰¢áˆ® á‰. SL-05A(áŠ¨ áˆŠáá‰± áŠá‰µ áˆˆ áŠá‰µ)\\n\\nğŸ“á‰.2 ğŸ‘‰áˆˆá‰¡\\xa0 áˆ˜á‹³áˆ…áŠ’á‹“áˆˆáˆ á‰¤á‰°/áŠ­áˆ­áˆµá‰²á‹«áŠ• áŠá‰µ áˆˆáŠá‰µ\\xa0 #á‹›áˆ_áˆáˆ 2áŠ› áá‰… á‰¢áˆ® á‰áŒ¥áˆ­.214\\n\\nğŸ‘áˆˆá‰¡\\xa0á‰…áˆ­áŠ•áŒ«áğŸ“²0973611819\\n\\n\\n\\n\\xa0\\xa0\\xa0\\xa0 ğŸ’§ğŸ’§ğŸ’§ğŸ’§\\n\\n\\n\\xa0\\xa0\\xa0 ğŸ“² 0909522840\\n\\xa0\\xa0\\xa0 ğŸ“² 0923350054\\n\\nğŸ”–\\nğŸ’¬\\xa0 á‰ Telegram áˆˆáˆ›á‹˜á‹ â¤µï¸ á‹­áŒ á‰€áˆ™\\n@shager_onlinestore\\n\\xa0 \\náˆˆá‰°áŒ¨áˆ›áˆª áˆ›á‰¥áˆ«áˆªá‹« á‹¨á‰´áˆŒáŒáˆ«áˆ áŒˆáƒá‰½áŠ•â¤µï¸\\nhttps://t.me/Shageronlinestore', 'ğŸ’¥Only baby 3in1 double bottle milk warmer,sterilizer,food steamer\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 á‹‹áŒ‹:-3000á‰¥áˆ­âœ…\\n\\nâŒá‹áˆµáŠ• ááˆ¬ áŠá‹ á‹«áˆˆá‹\\n\\nğŸ¢ áŠ á‹µáˆ«áˆ»\\xa0 á‰.1ğŸ‘‰ áˆµáˆª áŠ¤áˆ áˆ²á‰² áˆáˆ\\xa0 áˆáˆˆá‰°áŠ› áá‰… á‰¢áˆ® á‰. SL-05A(áŠ¨ áˆŠáá‰± áŠá‰µ áˆˆ áŠá‰µ)\\n\\nğŸ“á‰.2 ğŸ‘‰áˆˆá‰¡\\xa0 áˆ˜á‹³áˆ…áŠ’á‹“áˆˆáˆ á‰¤á‰°/áŠ­áˆ­áˆµá‰²á‹«áŠ• áŠá‰µ áˆˆáŠá‰µ\\xa0 #á‹›áˆ_áˆáˆ 2áŠ› áá‰… á‰¢áˆ® á‰áŒ¥áˆ­.214\\n\\nğŸ‘áˆˆá‰¡\\xa0á‰…áˆ­áŠ•áŒ«áğŸ“²0973611819\\n\\n\\n\\n\\xa0\\xa0\\xa0\\xa0 ğŸ’§ğŸ’§ğŸ’§ğŸ’§\\n\\n\\n\\xa0\\xa0\\xa0 ğŸ“² 0909522840\\n\\xa0\\xa0\\xa0 ğŸ“² 0923350054\\n\\nğŸ”–\\nğŸ’¬\\xa0 á‰ Telegram áˆˆáˆ›á‹˜á‹ â¤µï¸ á‹­áŒ á‰€áˆ™\\n@shager_onlinestore\\n\\xa0 \\náˆˆá‰°áŒ¨áˆ›áˆª áˆ›á‰¥áˆ«áˆªá‹« á‹¨á‰´áˆŒáŒáˆ«áˆ áŒˆáƒá‰½áŠ•â¤µï¸\\nhttps://t.me/Shageronlinestore', 'ğŸ’¥1pc stainless steel loaf pan\\n\\nğŸ‘á‰µáˆá‰\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 á‹‹áŒ‹:-á‰µáˆá‰:-800á‰¥áˆ­âœ…\\n\\nâŒá‹áˆµáŠ• ááˆ¬ áŠá‹ á‹«áˆˆá‹\\n\\nğŸ¢ áŠ á‹µáˆ«áˆ»\\xa0 á‰.1ğŸ‘‰ áˆµáˆª áŠ¤áˆ áˆ²á‰² áˆáˆ\\xa0 áˆáˆˆá‰°áŠ› áá‰… á‰¢áˆ® á‰. SL-05A(áŠ¨ áˆŠáá‰± áŠá‰µ áˆˆ áŠá‰µ)\\n\\nğŸ“á‰.2 ğŸ‘‰áˆˆá‰¡\\xa0 áˆ˜á‹³áˆ…áŠ’á‹“áˆˆáˆ á‰¤á‰°/áŠ­áˆ­áˆµá‰²á‹«áŠ• áŠá‰µ áˆˆáŠá‰µ\\xa0 #á‹›áˆ_áˆáˆ 2áŠ› áá‰… á‰¢áˆ® á‰áŒ¥áˆ­.214\\n\\nğŸ‘áˆˆá‰¡\\xa0á‰…áˆ­áŠ•áŒ«áğŸ“²0973611819\\n\\n\\xa0\\xa0\\xa0 ğŸ“² 0909522840\\n\\xa0\\xa0\\xa0 ğŸ“² 0923350054\\n\\nğŸ”–\\nğŸ’¬\\xa0 á‰ Telegram áˆˆáˆ›á‹˜á‹ â¤µï¸ á‹­áŒ á‰€áˆ™\\n@shager_onlinestore\\n\\xa0 \\náˆˆá‰°áŒ¨áˆ›áˆª áˆ›á‰¥áˆ«áˆªá‹« á‹¨á‰´áˆŒáŒáˆ«áˆ áŒˆáƒá‰½áŠ•â¤µï¸\\nhttps://t.me/Shageronlinestore', 'ğŸ“£Geemy Rechargable Hair\\xa0 Clipper\\n\\nâœ”ï¸ á‹¨á€áŒ‰áˆ­áŠ“ á‹¨á‚áˆ áˆ˜á‰áˆ¨áŒªá‹« á‰¶áŠ•á‹¶áˆµ\\nâœ”ï¸ á‰ á‰»áˆ­áŒ… á‹¨áˆšáˆ°áˆ« \\nâœ”ï¸ áŠ¨3-4 áˆ°á‹“á‰µ á‰»áˆ­áŒ… á‰°á‹°áˆ­áŒ áŠ¨240-300 á‹°á‰‚á‰ƒ\\xa0 á‹­áŒ á‰€áˆ™á‰ á‰³áˆ\\nâœ”ï¸ áˆ˜áˆˆá‹‹á‹ˆáŒ« 4 áŒ¥áˆ­áˆµ á‹«áˆˆá‹\\nâœ”ï¸ 2000mA á‰£á‰µáˆª\\nâœ”ï¸ á‰ á‹¨á‰µáŠ›á‹áˆ á‹¨áˆ°á‹áŠá‰µ áŠ­ááˆ á‹¨áˆšá‰ á‰…áˆ á€áŒ‰áˆ­áŠ• áˆ™áˆáŒ­áŒ­ áŠ á‹µáˆ­áŒ á‹­á‰†áˆ­áŒ£áˆá¤ á‰…áˆ­á…áˆ á‹«á‹ˆáŒ£áˆ\\nğŸ¤« á€áŒ‰áˆ­ á‰¤á‰µ á‹­á‹˜á‹á‰µ á‰¢áŒ á‰€áˆ™á‰ á‰µ á‹¨áˆšá‹«áŠ®áˆ«\\nâœ”ï¸ á‹¨á‰£á‰µáˆª áˆ˜áŒ áŠ• áˆ›áˆ³á‹« áˆµáŠ­áˆªáŠ• á‹«áˆˆá‹\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0 á‹‹áŒ‹:-2100 á‰¥áˆ­ğŸ’µ\\n\\nâŒá‹áˆµáŠ• ááˆ¬ áŠá‹ á‹«áˆˆá‹\\n\\nğŸ¢ áŠ á‹µáˆ«áˆ»\\xa0 á‰.1ğŸ‘‰ áˆµáˆª áŠ¤áˆ áˆ²á‰² áˆáˆ\\xa0 áˆáˆˆá‰°áŠ› áá‰… á‰¢áˆ® á‰. SL-05A(áŠ¨ áˆŠáá‰± áŠá‰µ áˆˆ áŠá‰µ)\\n\\nğŸ“á‰.2 ğŸ‘‰áˆˆá‰¡\\xa0 áˆ˜á‹³áˆ…áŠ’á‹“áˆˆáˆ á‰¤á‰°/áŠ­áˆ­áˆµá‰²á‹«áŠ• áŠá‰µ áˆˆáŠá‰µ\\xa0 #á‹›áˆ_áˆáˆ 2áŠ› áá‰… á‰¢áˆ® á‰áŒ¥áˆ­.214\\n\\nğŸ‘áˆˆá‰¡\\xa0á‰…áˆ­áŠ•áŒ«áğŸ“²0973611819\\n\\n\\xa0\\xa0\\xa0\\xa0 \\n\\xa0\\xa0\\xa0 ğŸ“² 0909522840\\n\\xa0\\xa0\\xa0 ğŸ“² 0923350054\\n\\nğŸ”–\\nğŸ’¬\\xa0 á‰ Telegram áˆˆáˆ›á‹˜á‹ â¤µï¸ á‹­áŒ á‰€áˆ™\\n@shager_onlinestore\\n\\xa0 \\náˆˆá‰°áŒ¨áˆ›áˆª áˆ›á‰¥áˆ«áˆªá‹« á‹¨á‰´áˆŒáŒáˆ«áˆ áŒˆáƒá‰½áŠ•â¤µï¸\\nhttps://t.me/Shageronlinestore', 'ğŸ’¥ kids water bottle\\n\\n\\xa0ğŸ‘‰áŒ áŠ•áŠ«áˆ« áŠ¥áŠ“ áŠ¥áˆ›á‹­áˆ°á‰ áˆ­\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0á‹‹áŒ‹:-450ml:-600á‰¥áˆ­âœ…\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 800ml:-650á‰¥áˆ­âœ…\\n\\nğŸ¢ áŠ á‹µáˆ«áˆ»\\xa0 á‰.1ğŸ‘‰ áˆ˜áŒˆáŠ“áŠ áˆµáˆª áŠ¤áˆ áˆ²á‰² áˆáˆ\\xa0 áˆáˆˆá‰°áŠ› áá‰… á‰¢áˆ® á‰. SL-05A(áŠ¨ áˆŠáá‰± áŠá‰µ áˆˆ áŠá‰µ)\\n\\nğŸ“á‰.2 ğŸ‘‰áˆˆá‰¡\\xa0 áˆ˜á‹³áˆ…áŠ’á‹“áˆˆáˆ á‰¤á‰°/áŠ­áˆ­áˆµá‰²á‹«áŠ• áŠá‰µ áˆˆáŠá‰µ\\xa0 #á‹›áˆ_áˆáˆ 2áŠ› áá‰… á‰¢áˆ® á‰áŒ¥áˆ­.214\\n\\n\\nğŸ‘áˆˆá‰¡\\xa0á‰…áˆ­áŠ•áŒ«áğŸ“²0973611819\\n\\n\\xa0\\xa0\\xa0\\xa0 \\n\\n\\n\\xa0\\xa0\\xa0 ğŸ“² 0909522840\\n\\xa0\\xa0\\xa0 ğŸ“² 0923350054\\n\\nğŸ”–\\nğŸ’¬\\xa0 á‰ Telegram áˆˆáˆ›á‹˜á‹ â¤µï¸ á‹­áŒ á‰€áˆ™\\n@shager_onlinestore\\n\\xa0 \\náˆˆá‰°áŒ¨áˆ›áˆª áˆ›á‰¥áˆ«áˆªá‹« á‹¨á‰´áˆŒáŒáˆ«áˆ áŒˆáƒá‰½áŠ•â¤µï¸\\nhttps://t.me/Shageronlinestore', 'ğŸ’¥4Pcs isolated lunch box with bag \\n\\nâš¡ï¸á‰ á‹áˆµáŒ¡ 4 á‹¨áˆáˆ³ á‹•á‰ƒá‹ˆá‰½ á‹¨á‹«á‹˜\\nâš¡ï¸leakpï¸roof\\nâš¡ï¸stainless still food grade materials\\nâš¡ï¸áˆáŒá‰¥á‹áŠ• áˆ³á‹«á‰€á‹˜á‰…á‹ áŠ¥áŠ•á‹°áˆá‰€ áˆšá‹«á‰†á‹­\\n\\xa0\\xa0\\xa0\\xa0\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 ğŸŸ á‰ á‰¢áˆ®á‹ á‹áˆµáŒ¥\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 ğŸµ áˆˆáˆáŒ†á‰½ á‰µáˆáˆ…áˆ­á‰µ á‰¤á‰µ\\n\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0 á‹‹áŒ‹:-3200á‰¥áˆ­âœ…\\n\\n\\nâœ–ï¸á‹áˆµáŠ• ááˆ¬ áŠá‹ á‹«áˆˆáŠ•\\n\\n\\nğŸ¢ áŠ á‹µáˆ«áˆ»\\xa0 á‰.1ğŸ‘‰ áˆ˜áŒˆáŠ“áŠ› áˆµáˆª áŠ¤áˆ áˆ²á‰² áˆáˆ\\xa0 áˆáˆˆá‰°áŠ› áá‰… á‰¢áˆ® á‰. SL-05A(áŠ¨ áˆŠáá‰± áŠá‰µ áˆˆ áŠá‰µ)\\n\\nğŸ“á‰.2 ğŸ‘‰áˆˆá‰¡\\xa0 áˆ˜á‹³áˆ…áŠ’á‹“áˆˆáˆ á‰¤á‰°/áŠ­áˆ­áˆµá‰²á‹«áŠ• áŠá‰µ áˆˆáŠá‰µ\\xa0 #á‹›áˆ_áˆáˆ 2áŠ› áá‰… á‰¢áˆ® á‰áŒ¥áˆ­.214\\n\\n\\nğŸ‘áˆˆá‰¡\\xa0á‰…áˆ­áŠ•áŒ«áğŸ“²0973611819\\n\\n\\n\\xa0\\xa0\\xa0\\n\\n\\xa0\\xa0\\xa0 ğŸ“² 0909522840\\n\\xa0\\xa0\\xa0 ğŸ“² 0923350054\\n\\nğŸ”–\\nğŸ’¬\\xa0 á‰ Telegram áˆˆáˆ›á‹˜á‹ â¤µï¸ á‹­áŒ á‰€áˆ™\\n@shager_onlinestore\\n\\xa0 \\náˆˆá‰°áŒ¨áˆ›áˆª áˆ›á‰¥áˆ«áˆªá‹« á‹¨á‰´áˆŒáŒáˆ«áˆ áŒˆáƒá‰½áŠ•â¤µï¸\\nhttps://t.me/Shageronlinestore']\n",
      "Input format is correct.\n",
      "Type of input: <class 'numpy.ndarray'>\n",
      "Contents of input: ['[MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK]']\n",
      "Error while explaining SHAP values: Input must be a list of strings.\n",
      "SHAP values could not be computed due to previous errors.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import shap\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "# Load your CSV data\n",
    "csv_file_path = '../data/cleaned_telegram_data.csv'  # Update this with your CSV file path\n",
    "data = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Clean the 'Message' column by dropping NaN values and converting to strings\n",
    "data['Message'] = data['Message'].dropna().astype(str)\n",
    "\n",
    "# Convert the cleaned column to a list of strings\n",
    "texts = data['Message'].tolist()\n",
    "\n",
    "# Check for any NaN or empty values after cleaning\n",
    "if any(pd.isna(texts)) or any(text == \"\" for text in texts):\n",
    "    print(\"There are still NaN or empty values in the texts.\")\n",
    "else:\n",
    "    print(\"All NaN and empty values have been cleaned from the texts.\")\n",
    "\n",
    "# Define your model name (change this to your specific model)\n",
    "model_name = \"dbmdz/bert-large-cased-finetuned-conll03-english\"  # Example model\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "# 1. SHAP Explanation\n",
    "# Prepare the function to predict token classification\n",
    "def predict_shap(texts):\n",
    "    # Ensure input is a list of strings\n",
    "    print(f\"Type of input: {type(texts)}\")\n",
    "    print(f\"Contents of input: {texts}\")\n",
    "\n",
    "    if isinstance(texts, list) and all(isinstance(text, str) for text in texts):\n",
    "        # Tokenize all texts at once, ensure it's a list of strings\n",
    "        inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predicted_classes = logits.argmax(dim=-1)  # Get predicted classes\n",
    "        \n",
    "        # Return predictions as a list of lists\n",
    "        return predicted_classes.tolist()  # Convert to list of lists\n",
    "    else:\n",
    "        raise ValueError(\"Input must be a list of strings.\")\n",
    "\n",
    "# Create a new explainer that directly works with the model's predictions\n",
    "explainer = shap.Explainer(predict_shap, tokenizer)\n",
    "\n",
    "# Limit the number of texts for SHAP to avoid memory issues\n",
    "num_samples = min(10, len(texts))  # Change 10 to any number you want to visualize\n",
    "\n",
    "# Get a sample of texts\n",
    "sample_texts = texts[:num_samples]  # Get a sample of texts\n",
    "print(f\"Sample input type: {type(sample_texts)}, Content: {sample_texts}\")\n",
    "\n",
    "# Ensure the sample input is indeed a list of strings\n",
    "if isinstance(sample_texts, list) and all(isinstance(text, str) for text in sample_texts):\n",
    "    print(\"Input format is correct.\")\n",
    "else:\n",
    "    print(\"Input format is incorrect.\")\n",
    "    raise ValueError(\"Input must be a list of strings.\")  # Raise error for debugging\n",
    "\n",
    "# Pass a slice of the data to the explainer\n",
    "shap_values = None  # Initialize shap_values to None\n",
    "try:\n",
    "    shap_values = explainer(sample_texts)  # Ensure this is a list of strings\n",
    "except ValueError as e:\n",
    "    print(f\"Error while explaining SHAP values: {e}\")\n",
    "\n",
    "# Check if shap_values was defined before attempting to visualize\n",
    "if shap_values is not None:\n",
    "    # Visualization of SHAP values\n",
    "    for i in range(num_samples):\n",
    "        shap.plots.text(shap_values[i])  # This will show which tokens contributed to the NER decision\n",
    "else:\n",
    "    print(\"SHAP values could not be computed due to previous errors.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All NaN and empty values have been cleaned from the texts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import shap\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "# Load your CSV data\n",
    "csv_file_path = '../data/cleaned_telegram_data.csv'  # Update this with your CSV file path\n",
    "data = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Clean the 'Message' column by dropping NaN values and converting to strings\n",
    "data['Message'] = data['Message'].dropna().astype(str)\n",
    "\n",
    "# Convert the cleaned column to a list of strings\n",
    "texts = data['Message'].tolist()\n",
    "\n",
    "# Check for any NaN or empty values after cleaning\n",
    "if any(pd.isna(texts)) or any(text == \"\" for text in texts):\n",
    "    print(\"There are still NaN or empty values in the texts.\")\n",
    "else:\n",
    "    print(\"All NaN and empty values have been cleaned from the texts.\")\n",
    "\n",
    "# Define your model name (change this to your specific model)\n",
    "model_name = \"dbmdz/bert-large-cased-finetuned-conll03-english\"  # Example model\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "# Initialize NER pipeline\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "class_names = [f\"Class {i}\" for i in range(len(model.config.id2label))]  # Update class names based on your model\n",
    "lime_explainer = LimeTextExplainer(class_names=class_names)\n",
    "\n",
    "# Use LIME to explain NER output\n",
    "def lime_predict_proba(texts):\n",
    "    output = []\n",
    "    for text in texts:\n",
    "        result = ner_pipeline(text)\n",
    "        proba = np.zeros(len(text))\n",
    "        for item in result:\n",
    "            # Simple binary marking of entities\n",
    "            proba[item['start']:item['end']] = 1  \n",
    "        output.append(proba)\n",
    "    return np.array(output)\n",
    "\n",
    "# Example text to explain\n",
    "text_example = \"John Smith went to the bank.\"  \n",
    "\n",
    "\n",
    "lime_explanation = lime_explainer.explain_instance(text_example, lime_predict_proba, num_features=6)\n",
    "\n",
    "# Visualize explanation\n",
    "lime_explanation.show_in_notebook(text=True)  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
